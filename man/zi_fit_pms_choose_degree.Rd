% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/zero_fit_pms.R
\name{zi_fit_pms_choose_degree}
\alias{zi_fit_pms_choose_degree}
\title{Fits and chooses a Hurdle conditional model with pms parametrization of degree <= a maximum degree.}
\usage{
zi_fit_pms_choose_degree(
  V,
  Y,
  left,
  right,
  max_uniform_degree,
  extra_regressors = NULL,
  extra_reg_pen_factors = NULL,
  value_only = TRUE,
  tol = 1e-08,
  maxit = 1e+05,
  seed = NULL,
  penalize_decider = function(X) {     ncol(X) >= nrow(X)/2 },
  nfits = 10,
  runs = 2,
  print_best_degree = FALSE
)
}
\arguments{
\item{V}{A matrix of 0/1s, equal to Y != 0.}

\item{Y}{A data matrix of the same size as \code{V}.}

\item{left}{An integer between 1 and \code{ncol(Y)}. The index of the variable to be fit.}

\item{right}{A vector of integers between 1 and \code{ncol(Y)} different from \code{left}. Indices of the "regressors".}

\item{max_uniform_degree}{A positive integer, the maximum degree for the Hurdle polynomials.}

\item{extra_regressors}{A matrix with the same number of rows as \code{V} and \code{Y}, extra regressors to be included in both regressions (conditional log odds/conditional mean). Defaults to \code{NULL}.}

\item{extra_reg_pen_factors}{A vector of non-negative numbers, defaults to \code{NULL}. Penalty factors for \code{extra_regressors}. If the main design matrix has \code{d} columns, \code{c(rep(1, d), extra_reg_pen_factors)} will be passed as the \code{penalty.factor} argument to \code{glmnet::glmnet()}. If \code{intercept == TRUE}, a \code{0} will also be prepended.}

\item{value_only}{If \code{TRUE}, returns the minimized negative log likelihood only. Defaults to \code{TRUE}.}

\item{tol}{A number, tolerance. Defaults to \code{1e-8}. Passed to \code{stats::glm()} for penalized logistic regressions, or as the \code{thresh} argument to \code{glmnet::glmnet()} for both logistic and linear regressions if penalized.}

\item{maxit}{An integer, the maximum number of iterations. Defaults to \code{100000}. Passed to \code{stats::glm()} for penalized logistic regressions, or to \code{glmnet::glmnet()} for both logistic and linear regressions if penalized.}

\item{seed}{A number, the random seed passed to \code{zi_fit_lm()} for both regressions (conditional log odds/conditional mean).}

\item{penalize_decider}{A logical or a function that takes a design matrix and returns a logical. Defaults to \code{function(X){ncol(X)>=nrow(X)/2}}. Used to decide whether to use penalized l2 (ridge) regression (if \code{TRUE}) when fitting each conditional distribution. Note that for either regression (conditional log odds/conditional mean), if the fits for unpenalized regressions are almost perfect, penalized regressions will be automatically used.}

\item{nfits}{A positive integer, defaults to \code{10}. Used for penalized regressions, as number of folds if \code{CV_BIC == TRUE} (\code{nfits} argument to \code{glmnet::cv.glmnet()}, with \code{nlambda} set to \code{100}), or the number of lambdas if \code{BIC == FALSE} (as the \code{nlambda} argument to \code{glmnet::glmnet()}).}

\item{runs}{A positive integer, the number of reruns. The fit with the maximum likelihood will be returned. Defaults to \code{2}.}

\item{print_best_degree}{A logical, whether to print the degree (1, ..., \code{max_uniform_degree}) that minimizes the BIC.}
}
\value{
If \code{value_only == TRUE}, returns the minimized negative log likelihood only. Otherwise, returns
  \item{nll}{A number, the minimized negative log likelihood.}
  \item{par}{A vector of length \code{4*length(right)+3}, the fitted parameters, in the other of: the intercept for the \code{a} (a scalar), linear coefficients on \code{V[,right]} for \code{a}, linear coefficients on \code{Y[,right]} for \code{a}, the intercept for the \code{b} (a scalar), linear coefficients on \code{V[,right]} for \code{b}, linear coefficients on \code{Y[,right]} for \code{b}.}
  \item{n}{An integer, the sample size.}
  \item{effective_df}{\code{4*length(right)+3}, the effective degree of freedom.}
}
\description{
Fits and chooses a Hurdle conditional model with pms parametrization of degree <= a maximum degree.
}
\details{
A Hurdle conditional model with pms parametrization for the \code{left} node given those in \code{right} has log density with respect to the sum of the Lebesgue measure and a point mass at 0 equal to (in terms of \code{y})
\eqn{\log(1-p)}{log(1-p)} if \code{y == 0}, or \eqn{\log(p)-(y-mu)^2/2/sigmasq}{log(p)-(y-mu)^2/2/sigmasq} otherwise. That is, it is a mixture of a binomial with probability of success \code{p} and a Gaussian with conditional mean \code{mu} and conditional variance \code{sigmasq}.
Here \code{sigmasq} is assumed constant, and parameters \code{log(p/(1-p))} and \code{mu} are Hurdle polynomials, i.e. polynomials in the values for \code{right} and their indicators.
This function thus fits such a model using \code{Y[,left]}, \code{Y[,right]} and \code{V[,right] = (Y[,right] != 0)}, using a logistic for the log odds \code{log(p/(1-p))} and a linear regression for \code{mu}.

Writing \code{Yo <- Y[,right]}, a Hurdle polynomial in parents \code{Yo} is a polynomial in \code{Yo} and their 0/1 indicators \code{Vo}.
The degree of a term in a Hurdle polynomial is the number of \code{V} terms plus the sum of the degrees of the \code{Y} terms. For example, \code{Y1^2 * V2 * Y3^3 * V4 * V5} has degree equal to 2+1+3+1+1=8.
Given a degree, the design matrix thus includes all possible terms with degree less than or equal to the specified degree.
For example, if \code{Vo} and \code{Yo} has two columns and if we choose degree \code{2}, the design matrix has columns \code{V1}, \code{V2}, \code{V1*V2}, \code{Y1}, \code{Y2}, \code{Y1*Y2}, \code{Y1^2}, \code{Y2^2}, \code{Y1*V2}, \code{Y2*V1}. Note that terms like \code{V1*Y1} are not included as it is equivalent to \code{Y1}.

This function fits models using Hurdle polynomials with degrees 1, 2, ..., \code{max_uniform_degree}, and automatically chooses the degree that minimizes the BIC.
It is equivalent to calling \code{zi_fit_pms()} with all degree arguments equal to \code{d}, with \code{d} in 1, ..., \code{max_uniform_degree}, and returning the one with the smallest BIC.
}
\examples{
m <- 3; n <- 1000
adj_mat <- make_dag(m, "complete")
dat <- gen_zero_dat(1, "pms", adj_mat, n, k_mode=1, min_num=10, gen_uniform_degree=1)
extra_regressors <- matrix(rnorm(n * 4), nrow=n)
extra_reg_pen_factors <- c(1, 2, 3, 4) / sum(c(1, 2, 3, 4))
zi_fit_pms_choose_degree(dat$V, dat$Y, 3, 1:2, max_uniform_degree=2L,
    extra_regressors=extra_regressors, extra_reg_pen_factors=extra_reg_pen_factors,
    value_only=TRUE, print_best_degree=TRUE)
zi_fit_pms_choose_degree(dat$V, dat$Y, 3, 1:2, max_uniform_degree=2L,
    extra_regressors=extra_regressors, extra_reg_pen_factors=extra_reg_pen_factors,
    value_only=FALSE, print_best_degree=TRUE)
}
